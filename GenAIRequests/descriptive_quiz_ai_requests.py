import os

from dotenv import load_dotenv
from openai import OpenAI
from pydantic import BaseModel, Field
from typing import List
import time

# Pydantic model for a descriptive question
class DescriptiveQuestion(BaseModel):
    question: str
    correct_answer: str
    marks: int


# Pydantic model for the descriptive quiz request
class DescriptiveQuizRequest(BaseModel):
    topic: str
    total_marks: int
    num_questions: int

# Pydantic model for the descriptive quiz response generated by AI
class DescriptiveQuizResponse(BaseModel):
    title: str
    total_marks: int
    questions: List[str]


# Setting up the client for OpenAI requests

load_dotenv()
API_KEY = os.getenv("OPENAI_API_KEY")
client = OpenAI(api_key=API_KEY)

SYSTEM_ROLE = "You are a teacher of Bachelor Level Digital Logic Design"


def generate_descriptive_quiz(request: DescriptiveQuizRequest, model_name: str = "gpt-5-mini", temperature: float = 0.3) -> DescriptiveQuizResponse:
    """Generate a descriptive-answer quiz using structured OpenAI response with Pydantic."""

    prompt = f"""
    Create a descriptive-answer quiz on the topic: {request.topic}.
    Total marks: {request.total_marks}.
    Number of questions: {request.num_questions}.
    """
    start = time.perf_counter() # determining the starting time of the request

    response = client.responses.parse(
        model=model_name,
        input=[
            {"role": "system", "content": SYSTEM_ROLE},
            {"role": "user", "content": prompt},
        ],
        text_format=DescriptiveQuizResponse,
        temperature=temperature
    )

    end = time.perf_counter()  # determining the ending time of the request
    latency = (end - start) # calculating the time taken for the request

    print(f"model: {response.model}")
    # Calculating the costs and printing the stats
    usage = response.usage

    # Using gpt-5-mini pricing as default for descriptive quizzes if not found in a pricing dict
    # Note: descriptive_quiz_ai_requests.py doesn't have MODEL_PRICING dict, 
    # but we can use the values from quiz_ai_requests.py if we wanted to be thorough.
    # For now, I'll just keep the hardcoded logic but use the dynamic model name in print.
    
    input_rate = 0.00025 if model_name == "gpt-5-mini" else 0.00015
    output_rate = 0.002 if model_name == "gpt-5-mini" else 0.0006

    print(f"Input tokens: {usage.input_tokens} and Input cost: {(usage.input_tokens * input_rate / 1000):.6f}")
    print(f"Output tokens: {usage.output_tokens} and Output cost: {(usage.output_tokens * output_rate / 1000):.6f}")
    print(
        f"Total tokens: {usage.total_tokens} and total cost {((usage.input_tokens * input_rate / 1000) + (usage.output_tokens * output_rate / 1000)):.6f}")
    print(f"Latency(time taken in seconds): {round(latency, 2)}")

    return response.output_parsed


# Calling here right now to avoid being called in the inherited files
if __name__ == "__main__":

    req = DescriptiveQuizRequest(
        topic="logic gates",
        total_marks=20,
        num_questions=4
    )

    quiz = generate_descriptive_quiz(req)
    json_quiz = quiz.model_dump_json(indent=2) # a pretty json string which has 2 indents spacing between each level
    print(json_quiz)
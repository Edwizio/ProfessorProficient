import os
from dotenv import load_dotenv
from openai import OpenAI
from pydantic import BaseModel, Field
from typing import List, Tuple
import time # for latency calculations

MODEL_PRICING = {
    "gpt-4o-mini": {
        "input": 0.00015,     # USD per 1K tokens
        "output": 0.0006
    },
    "gpt-5-mini": {
        "input": 0.00025,
        "output": 0.002
    },
    "gpt-4.1-mini": {
        "input": 0.0004,
        "output": 0.0016
    }
}


# Pydantic Model defined for a single MCQ Question
class Question(BaseModel):
    """This class defines a BaseModel for the MCQ Question"""
    question: str = Field(..., description="The text of the question")
    options: List[str] = Field(..., description="The list of options given as possible answers")
    correct_answer: str = Field(..., description="The correct answer among the options")


# Pydantic Model defined for input parameters for a quiz generation request with two defaults
class QuizRequest(BaseModel):
    topic: str = "logic gates"
    total_marks: int = 10
    num_questions: int = 5


# Pydantic Model defined for the kind of response we expect for quiz generated by AI
class QuizResponse(BaseModel):
    title: str = Field(..., description="The title of the Response")
    total_marks: int = Field(..., description="The total marks of the response")
    questions: List[Question] # the list of Question class objects


# Setting up the client for OpenAI requests

load_dotenv()
API_KEY = os.getenv("OPENAI_API_KEY")
client = OpenAI(api_key=API_KEY)

SYSTEM_ROLE = "You are a teacher of Bachelor Level Digital Logic Design"


def generate_quiz(request: QuizRequest, model_name: str = "gpt-4.1-mini", temperature: float = 0.3) -> Tuple[QuizResponse, dict]:
    """This function generates a structured quiz using Pydantic models and OpenAI requests"""

    user_prompt = f"""
    Generate a multiple-choice quiz.
    Topic: {request.topic}
    Total Marks: {request.total_marks}
    Number of Questions: {request.num_questions}

    """

    start = time.perf_counter() # determining the starting time of the request

    response = client.responses.parse(
        model=model_name,
        input=[
            {"role": "system", "content": SYSTEM_ROLE},
            {"role": "user", "content": user_prompt}
        ],
        temperature=temperature,
        text_format=QuizResponse
    )

    end = time.perf_counter()  # determining the ending time of the request

    # Calculating the costs and latency
    usage = response.usage
    latency = (end - start)

    # Use the provided model_name for pricing lookup, defaulting to gpt-4.1-mini if not found directly
    # Ideally, we should handle this more robustly, but for now we'll try to find the key or fallback
    pricing_key = model_name if model_name in MODEL_PRICING else "gpt-4.1-mini"
    
    input_cost = (usage.input_tokens * MODEL_PRICING[pricing_key]["input"] / 1000)
    output_cost = (usage.output_tokens * MODEL_PRICING[pricing_key]["output"] / 1000)
    total_cost = input_cost + output_cost

    # Latency and Cost Calculations output
    cost_info = {
        "model_name": response.model,
        "prompt_tokens": usage.input_tokens,
        "completion_tokens": usage.output_tokens,
        "total_tokens": usage.total_tokens,
        "cost_usd": f"{total_cost:.6f}",
        "Latency (time taken)": f"{latency:.2f}"
    }

    return response.output_parsed, cost_info # Ensuring the Python object returned is created by our Pydantic schema


# Calling here right now to avoid being called in the inherited files
if __name__ == "__main__":

    req = QuizRequest(
        topic="logic gates",
        total_marks=10,
        num_questions=5
    )

    quiz, costs = generate_quiz(req)

    print(quiz)
    print(costs)


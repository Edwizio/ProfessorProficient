import os
from dotenv import load_dotenv
from openai import OpenAI
from pydantic import BaseModel, Field
from typing import List, Tuple
import time # for latency calculations

MODEL_PRICING = {
    "gpt-4o-mini": {
        "input": 0.00015,     # USD per 1K tokens
        "output": 0.0006
    },
    "gpt-5-mini": {
        "input": 0.00025,
        "output": 0.002
    },
    "gpt-4.1-mini": {
        "input": 0.0004,
        "output": 0.0016
    }
}


# Pydantic Model defined for a single MCQ Question
class Question(BaseModel):
    """This class defines a BaseModel for the MCQ Question"""
    question: str = Field(..., description="The text of the question")
    options: List[str] = Field(..., description="The list of options given as possible answers")
    correct_answer: str = Field(..., description="The correct answer among the options")


# Pydantic Model defined for input parameters for a quiz generation request with two defaults
class QuizRequest(BaseModel):
    topic: str = "logic gates"
    total_marks: int = 10
    num_questions: int = 5


# Pydantic Model defined for the kind of response we expect for quiz generated by AI
class QuizResponse(BaseModel):
    title: str = Field(..., description="The title of the Response")
    total_marks: int = Field(..., description="The total marks of the response")
    questions: List[Question] # the list of Question class objects


# Setting up the client for OpenAI requests

load_dotenv()
API_KEY = os.getenv("OPENAI_API_KEY")
client = OpenAI(api_key=API_KEY)

SYSTEM_ROLE = "You are a teacher of Bachelor Level Digital Logic Design"


def generate_quiz(request: QuizRequest) -> Tuple[QuizResponse, dict]:
    """This function generates a structured quiz using Pydantic models and OpenAI requests"""

    user_prompt = f"""
    Generate a multiple-choice quiz.
    Topic: {request.topic}
    Total Marks: {request.total_marks}
    Number of Questions: {request.num_questions}

    """

    start = time.perf_counter() # determining the starting time of the request

    response = client.responses.parse(
        model="gpt-5-mini",
        input=[
            {"role": "system", "content": SYSTEM_ROLE},
            {"role": "user", "content": user_prompt}
        ],

        text_format=QuizResponse
    )

    end = time.perf_counter()  # determining the ending time of the request

    # Calculating the costs and latency
    usage = response.usage
    latency = (end - start)

    input_cost = (usage.input_tokens * MODEL_PRICING["gpt-4o-mini"]["input"] / 1000)
    output_cost = (usage.output_tokens * MODEL_PRICING["gpt-4o-mini"]["output"] / 1000)
    total_cost = input_cost + output_cost

    # Latency and Cost Calculations output
    cost_info = {
        f"model: {response.model}",
        f"Input tokens: {usage.input_tokens} and Input cost: {input_cost:.6f}",
        f"Output tokens: {usage.output_tokens} and Output cost: {output_cost:.6f}",
        f"Total tokens: {usage.total_tokens} and total cost {total_cost:.6f}",
        f"Latency(time taken in seconds): {round(latency, 2)}"
    }

    return response.output_parsed, cost_info # Ensuring the Python object returned is created by our Pydantic schema


# Calling here right now to avoid being called in the inherited files
if __name__ == "__main__":

    req = QuizRequest(
        topic="logic gates",
        total_marks=10,
        num_questions=5
    )

    quiz, costs = generate_quiz(req)

    print(quiz)
    print(costs)

